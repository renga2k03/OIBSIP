# -*- coding: utf-8 -*-
"""spamorham.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10nMhMWiuf8BY1Sxo-tx91xCBfl4ll8aX

# **Email Spam Detection**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn import svm
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder

"""## *Extracting the dataset from csv file*"""

mail_dataset = pd.read_csv('spam.csv', encoding='latin-1')
mail_dataset.head(15)

"""## *Checking for null values and encoding data*"""

mail_dataset.info()
mail_dataset.describe()
encoder = LabelEncoder()
mail_dataset['v1'] = encoder.fit_transform(mail_dataset['v1'])
mail_dataset

"""## *Removing unnecessary columns*"""

mail_dataset.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis=1, inplace=True)
mail_dataset

"""## *Analyzing the amount of spams and hams*"""

spam_count = mail_dataset[mail_dataset['v1'] == 1].shape[0]
ham_count = mail_dataset[mail_dataset['v1'] == 0].shape[0]
plt.pie([spam_count, ham_count], labels=['Spam', 'Ham'], autopct='%1.1f%%', startangle=90)
plt.show()

"""## *Preprocessing the texts*"""

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
lemma = WordNetLemmatizer()
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')
def lemmatization(sent):
    sent = sent.lower()
    sent = re.sub('[^a-zA-Z]',' ',sent)
    words = nltk.word_tokenize(sent)
    new_sent = ""
    for word in words:
        if word not in set(stopwords.words('english')):
            new_sent += lemma.lemmatize(word) + " "
    return new_sent
mail_dataset['v2'] = mail_dataset['v2'].apply(lemmatization)
mail_dataset

"""## *Extracting the features and target to split the dataset for training and testing*"""

feature = mail_dataset['v2']
target = mail_dataset['v1']
feature_train, feature_test, target_train, target_test = train_test_split(feature, target, test_size=0.2, random_state=3)

"""## *Applying TF-IDF Vectorizer on the training features*"""

tfidf = TfidfVectorizer(max_features=3000)
feature_train = tfidf.fit_transform(feature_train).toarray()
feature_test = tfidf.transform(feature_test).toarray()
feature_train
feature_test

"""## *Training the SVM model and performing predictions with test dataset*"""

model = svm.SVC()
model.fit(feature_train, target_train)
target_pred = model.predict(feature_test)
print(target_test, target_pred)

"""## *Estimating the accuracy of model in classification*"""

accuracy = accuracy_score(target_test, target_pred)
print("Accuracy of the classification model is : {:.2f} %".format(accuracy * 100))